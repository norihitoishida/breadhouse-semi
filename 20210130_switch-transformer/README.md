# 概要
- 1本目: TransformerのMixture of Expertsモデル

### 1. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
|Topic|Description|
|---|---|
|タイトル|Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity|
|日付|2021/01/11|
|著者|W Fedus, B Zoph, N Shazeer|
|所属|Google Brain|
|リンク|[arXiv:2101.03961 [cs.LG]](https://arxiv.org/abs/2101.03961)|
|概要|TransformerのMixture of Expertsモデル|
|備考||
